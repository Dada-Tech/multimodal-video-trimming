# -*- coding: utf-8 -*-
"""MultiModal_Video_Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Dada-Tech/multimodal-video-summarization-educational-content/blob/main/MultiModal_Video_Summarization.ipynb

# Setup

## Configuration
"""

import os
import subprocess

notebook_mode = "auto"
colab_mode = True  # for special requirements.txt without numpy/pandas

# Check for Notebook mode
notebook_mode = (
    True if 'COLAB_GPU' in os.environ else False) if notebook_mode == "auto" else notebook_mode

print(f"""Notebook Mode: {notebook_mode}""")


def print_section(message):
    """Prints a section separator with a custom message embedded.

    Args:
      message: The message to embed within the separator.
    """
    separator_length = 40
    separator_char = "="

    # Calculate padding for the message
    message_length = len(message)
    padding = (separator_length - message_length - 2) // 2
    padding = max(padding, 0)

    # Separator line
    top_line = separator_char * separator_length

    # Message line
    message_line = separator_char * padding + " " + message + " " + separator_char * padding
    if len(message_line) == 39:
        message_line += separator_char
    message_line = message_line[:separator_length]

    print(top_line)
    print(message_line)
    print(top_line, '\n')


def print_info(message, preview=None, max_length=80):
    print(f"""\n=== {message}\n""")
    if preview:
        print("--- Preview:")
        print(preview[:max_length] + "..." if len(
            preview) > max_length else preview)


def notebook_mode_print(message_or_df):
    if notebook_mode:
        display(message_or_df) if isinstance(message_or_df,
                                             pd.DataFrame) else print(
            message_or_df)


def ts_to_s(timestamp):
    """Converts a timestamp string in HH:MM:SS.mmm format to seconds.

    Args:
        timestamp: The timestamp string in HH:MM:SS.mmm format.

    Returns:
        The timestamp in seconds as a float.
    """
    hours, minutes, seconds_milliseconds = re.split(r':', timestamp)
    seconds, milliseconds = seconds_milliseconds.split('.')

    # Convert to seconds
    total_seconds = int(hours) * 3600 + int(minutes) * 60 + int(seconds) + int(
        milliseconds) / 1000.0

    return total_seconds


def generate_experiment_filename(id_token, base, filename_without_extension,
                                 extension):
    m1w = hyperparameters["metric_1"]["weight"]
    m2w = hyperparameters["metric_2"]["weight"]
    mdt = hyperparameters["deletion_metric"]["threshold"]

    experiment_name = f"_m1w-{m1w}_m2w-{m2w}_mdt-{mdt}_{id_token}"
    return os.path.join(base,
                        filename_without_extension + experiment_name + "." + extension)


# Define the function to count words while removing punctuation
def count_words_without_punctuation(sentence):
    words = word_tokenize(sentence)

    # Filter out punctuation
    words = [word for word in words if word not in string.punctuation]
    return len(words)


"""## Installation & Setup"""

if notebook_mode:
    print_section("installing deps")

    requirements_txt = "requirements-colab.txt" if colab_mode else "requirements.txt"

    # download requirements.txt from repository
    subprocess.run(["curl", "-O",
                    "https://raw.githubusercontent.com/Dada-Tech/multimodal-video-trimming/main/" + requirements_txt],
                   check=True)

    subprocess.check_call(
        ['python', '-m', 'pip', 'install', '--no-cache-dir', '-r',
         requirements_txt])

    print_info("installation done.")
else:
    print_info("skipping installation")

from pydantic import BaseModel, validator, conint, confloat, ValidationError
from enum import Enum
import argparse


class AutoSummary(BaseModel):
    summary_length_percentage: confloat(ge=0.2, le=0.5)
    min_summary_length: conint(ge=30, le=60)
    max_summary_length: conint(ge=100, le=3000)


class DeletionMetric(BaseModel):
    threshold: confloat(ge=0.05, le=0.5)


class Metric1(BaseModel):
    model_size: str
    weight: confloat(ge=0, le=1.0)
    min_words: conint(ge=50, le=1000)


class Metric2(BaseModel):
    weight: confloat(ge=0, le=1.0)
    min_scene_len: conint(ge=15, le=9000)
    threshold: conint(ge=10, le=50)


class Hyperparameters(BaseModel):
    auto_summary: AutoSummary
    deletion_metric: DeletionMetric
    metric_1: Metric1
    metric_2: Metric2  # Add Metric2 with constraints


"""## Inputs & Hyperparameters

### Auto Summary

*   **`summary_length_percentage`**: 0.3
    *   Determines the target length of the summary as a percentage of the original text length.
*   **`min_summary_length`**: 30
    *   Sets the minimum number of words (or tokens) allowed in the summary.
*   **`max_summary_length`**: 600
    *   Sets the maximum number of words (or tokens) allowed in the summary.

### Deletion Metric

*   **`threshold`**: 0.3
    *   Deleting the bottom **`n`** percent of scores.

### Metric 1
*   **`weight`**: Values between `0` and `1.0`
    *   Controls the influence of this metric on the final score
*   **`model_size`**: either `base` or `large`
    *   Controls which model used for text tasks for this metric.
*   **`min_words`**: minimum number of words for this
 metric to be used. Prevents this text-based metric from being used on cinematic type videos.

### Metric 2

*   **`weight`**: Values between `0` and `1.0`
    *   Controls the influence of this metric on the final score
*   **`min_scene_len`**: Values between `15` and `9000`
    *   Defines the minimum number of frames required for a scene to be considered.
    *   Ensures scenes are at least half a second long (15 frames) and at most 5 minutes (9000 frames at 30fps).
*   **`threshold`**: Values between `10` and `50`
    *   Sets the sensitivity for scene detection.
    *   Lower values detect more cuts, while higher values make detection stricter.
"""

if notebook_mode:
    video_input = "dataset/teamwork in the classroom.mov"
    video_output = "dataset/teamwork in the classroom_skimmed.mov"

    # video_input = "dataset/uGu_10sucQo.mp4"
    # video_output = "dataset/uGu_10sucQo_skimmed.mp4"

    experiment_mode = False  ## If experiment mode is True, you must run SRT File Gen block
    skip_nlp_downloads = False

    export_original_text = False
    export_trimmed_text = False
    export_summarized_text = False

    video_export_max_length_seconds = 0  # set develop video max length to export a shortened version of the multimedia

    # original was max_length=150, min_length=30
    hyperparameters = {
        "auto_summary": {
            "summary_length_percentage": 0.3,
            "min_summary_length": 30,
            "max_summary_length": 600
        },
        "deletion_metric": {
            "threshold": 0.2
        },
        "metric_1": {
            "model_size": "base",
            "weight": 1,
            "min_words": 50
        },
        "metric_2": {
            "weight": 0.3,
            "min_scene_len": 15,
            "threshold": 25
        }
    }

else:
    # Define the argparse parser
    parser = argparse.ArgumentParser(
        description="Process video and hyperparameters.")

    # Define the arguments for the inputs
    parser.add_argument("--video_input", "-i", type=str, required=True,
                        help="Path to the video input file")
    parser.add_argument("--video_output", "-o", type=str, default=None,
                        help="Path to save the output video")
    parser.add_argument("--experiment_mode", "-exp", action="store_true",
                        help="Run the project in experiment mode. (No video exports, just timestamps)")
    parser.add_argument("--video_export_max_length_seconds", type=int,
                        default=0,
                        help="Maximum length of the video to export (in seconds)")
    parser.add_argument("--export_original_text", action="store_true",
                        help="Export original text (from video)")
    parser.add_argument("--export_trimmed_text", action="store_true",
                        help="Export trimmed text")
    parser.add_argument("--export_summarized_text", action="store_true",
                        help="Export summarized text")
    parser.add_argument("--skip_nlp_downloads", action="store_true",
                        help="skip nltk library downloads")

    # Hyperparameters as individual arguments
    parser.add_argument("--auto_summary_summary_length_percentage", type=float,
                        default=0.3, help="Summary length as a percentage")
    parser.add_argument("--auto_summary_min_summary_length", type=int,
                        default=30, help="Minimum summary length")
    parser.add_argument("--auto_summary_max_summary_length", type=int,
                        default=600, help="Maximum summary length")

    parser.add_argument("--deletion_metric_threshold", type=float, default=0.2,
                        help="Threshold for deletion metric")

    parser.add_argument("--metric_1_model_size", type=str,
                        choices=["base", "large"], default="base",
                        help="Model size for metric 1")
    parser.add_argument("--metric_1_weight", type=float, default=1.0,
                        help="Weight for metric 1 (contribution to final score)")
    parser.add_argument("--metric_1_min_words", type=int, default=50,
                        help="Minimum number of words to determine this metric applicable")

    parser.add_argument("--metric_2_weight", type=float, default=0.3,
                        help="Weight for metric 2 (contribution to final score)")
    parser.add_argument("--metric_2_min_scene_len", type=int, default=15,
                        help="Minimum scene length for metric 2")
    parser.add_argument("--metric_2_threshold", type=int, default=25,
                        help="Threshold for scene detection for metric 2")

    # Parse arguments
    args = parser.parse_args()

    # Now you can use the parsed arguments
    video_input = args.video_input
    experiment_mode = args.experiment_mode
    skip_nlp_downloads = args.skip_nlp_downloads
    video_export_max_length_seconds = args.video_export_max_length_seconds
    video_output = args.video_output
    export_original_text = args.export_original_text
    export_trimmed_text = args.export_trimmed_text
    export_summarized_text = args.export_summarized_text

    hyperparameters = {
        "auto_summary": {
            "summary_length_percentage": args.auto_summary_summary_length_percentage,
            "min_summary_length": args.auto_summary_min_summary_length,
            "max_summary_length": args.auto_summary_max_summary_length
        },
        "deletion_metric": {
            "threshold": args.deletion_metric_threshold
        },
        "metric_1": {
            "model_size": args.metric_1_model_size,
            "weight": args.metric_1_weight,
            "min_words": args.metric_1_min_words
        },
        "metric_2": {
            "weight": args.metric_2_weight,
            "min_scene_len": args.metric_2_min_scene_len,
            "threshold": args.metric_2_threshold
        }
    }

if experiment_mode:
    print_section("Experiment Mode")

# Validate Hyperparameters
try:
    validated_hyperparameters = Hyperparameters(**hyperparameters)
except ValidationError as e:
    print(f"Hyperparameter validation error: {e}")

    print_info("exiting...")
    os._exit(1)

"""## Imports


"""

print_info("importing...")

import os
import numpy as np
import pandas as pd
import tarfile
import gdown
import re
from functools import reduce
import subprocess
import json

# ML General
from datasets import load_dataset
import torch
import torchaudio
import torch.nn.functional as F
from transformers import LEDTokenizer, LEDForConditionalGeneration
from sentence_transformers import SentenceTransformer

# Text
import string
import pytextrank
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import spacy
import srt

# Audio
import whisperx
import silero_vad
from silero_vad import load_silero_vad, read_audio, get_speech_timestamps
from pydub import AudioSegment

# Video
import ffmpeg
from scenedetect import detect, ContentDetector

print_info("importing done")

if not skip_nlp_downloads:
    print_info("downloading NLTK libraries...")

    nltk.download('punkt')
    nltk.download('punkt_tab')

    # Load the spaCy model
    spacy.cli.download("en_core_web_sm")
    sp = spacy.load('en_core_web_sm')

    print_info("downloading done")

"""## Variables"""

full_base = os.path.dirname(video_input)
path_dataset = full_base
filename = os.path.basename(video_input)
filename_without_extension = os.path.splitext(filename)[0]
filename_video_extension = video = os.path.splitext(video_input)[1]

# Video Input
filename_video_input = filename

# Output Filenames
filename_subtitles_output = filename_without_extension + ".srt"
filename_audio_output = filename_without_extension + ".wav"
filename_audio_output_skimmed = filename_without_extension + "_skimmed.wav"

# Text Output Filenmes
filename_paragraph_original = os.path.join(full_base,
                                           filename_without_extension + "_paragraph_original.txt")
filename_paragraph_trimmed = os.path.join(full_base,
                                          filename_without_extension + "_paragraph_trimmed.txt")
filename_paragraph_summarized = os.path.join(full_base,
                                             filename_without_extension + "_paragraph_summarized.txt")

# Experiment CSV
filename_experiment_sentences = generate_experiment_filename("sentences",
                                                             full_base,
                                                             filename_without_extension,
                                                             "csv")
filename_experiment_hyperparameters = generate_experiment_filename(
    "hyperparameters", full_base, filename_without_extension, "json")

# Output
subtitles_output = os.path.join(full_base, filename_subtitles_output)
audio_output = os.path.join(full_base, filename_audio_output)
audio_output_skimmed = os.path.join(full_base, filename_audio_output_skimmed)

if video_output:
    filename_video_output_skimmed = os.path.basename(video_output)
    video_output_skimmed = video_output
else:
    filename_video_output_skimmed = filename_without_extension + "_skimmed" + filename_video_extension
    video_output_skimmed = os.path.join(full_base,
                                        filename_video_output_skimmed)

video = ''
audio = ''
subtitles = ''
sentences = ''

"""## Functions"""


def drop_if_exists(df, col_name):
    """Drops a column from a DataFrame if it exists
    Args:
      df: The pandas DataFrame to modify.
      col_name: The name of the column to drop and insert.
    """
    if col_name in df.columns:
        df.drop(col_name, axis=1, inplace=True)


def paragraph_to_file(text, filename):
    try:
        # Process the text with spaCy to split into sentences
        doc = sp(text)

        sentences = [sent.text.strip() for sent in doc.sents]

        # Write each sentence to the file
        with open(filename, 'w') as file:
            for sentence in sentences:
                file.write(sentence + "\n")

        print(f"Export Successful: {filename}")
    except Exception as e:
        print(f"An error occurred: {e}")


"""## Datasets

- teamwork in the classroom.mov - `190MB`
- flipped learning basics.mov - `380MB`
- assessing students without exams.mov - `830MB`
"""

if notebook_mode:
    from google.colab import files

    # Google Drive Dataset Location
    folder_id = '1k7DLJPl1xz9lpU4l3dZYtPe1XawhrXeC'  # taken from drive.google.com/drive/u/1/folders/1k7D...(this part)
    gdown.download_folder(id=folder_id, quiet=False, use_cookies=False)

"""# Preprocessing - Audio"""

print_section("Preprocessing")

"""## Audio - Extract"""

# Extract audio (wav) from video
# !ffmpeg -y -i "$video_input" -vn -acodec pcm_s16le -ar 44100 -ac 2 "$audio_output"
print_info("extracting audio from video")

subprocess.run(
    ['ffmpeg', '-y', '-i', video_input, '-vn', '-acodec', 'pcm_s16le', '-ar',
     '44100', '-ac', '2', audio_output], check=True)
# subprocess.run(["ffmpeg", '-y', '-i', video_input, '-vn', '-acodec', 'pcm_s16le', '-ar', '44100', '-ac', '2', audio_output], check=True, capture_output=True)

"""## Audio - SRT File Generation

##### Time Taken: ~4min

SRT  
each **`subtitle`** in the subtitles array has the following properties:

1. **`index`**
   - The sequential number of the subtitle within the SRT file.
   - `1`, `2`, `3`, etc. (Integer)
2. **`start`**
   - The time (in milliseconds) when the subtitle should appear on the screen.
   - `00:00:05,000` (String representing HH:MM:SS,SSS)
3. **`end`**
   - The time (in milliseconds) when the subtitle should disappear from the screen.
   - `00:00:10,000` (String representing HH:MM:SS,SSS)
4. **`content`**
   - The actual text of the subtitle that will be displayed.
   - "Hello, world!" (String)
5. **`proprietary`**
   - This field holds any additional data or formatting specific to the SRT file or software used to create it. Often empty and can usually be ignored.
   - `''` (Empty string, or sometimes contains specific formatting codes)
"""


def seconds_to_srt_timestamp(seconds):
    """
    Extract hours, minutes, seconds, and milliseconds
    from a given number of seconds.
    """

    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    seconds = seconds % 60
    milliseconds = int((seconds - int(seconds)) * 1000)

    # Format as HH:MM:SS,MS
    return f"{hours:02}:{minutes:02}:{int(seconds):02},{milliseconds:03}"


# Select device (GPU if available, otherwise CPU)
language = "en"

from multiprocessing import Queue

# GPU
if torch.cuda.is_available():
    device = "cuda"
    compute_type = "float32"
    batch_size = 16
    model_whisperx = "base"

    print_info(f"""Generating SRT File with {device}...""")
else:
    device = "cpu"
    compute_type = "int8"
    batch_size = 1
    model_whisperx = "tiny"

    queue = Queue(maxsize=200)

    print_info(f"""WARNING: Generating SRT File with {device}...""")

# Model WhisperX
model = whisperx.load_model(model_whisperx, device=device, language=language,
                            compute_type=compute_type)  # Choose "base" or "large" model

# Transcribe audio
aligned_segments = model.transcribe(audio_output, batch_size=batch_size)

# Align with forced alignment
alignment_model, metadata = whisperx.load_align_model(
    language_code=aligned_segments["language"], device=device)
aligned_segments = whisperx.align(aligned_segments["segments"], alignment_model,
                                  metadata, audio_output, device)

if not experiment_mode:
    # Generate SRT file with aligned sentences
    with open(subtitles_output, "w") as f:
        for i, segment in enumerate(aligned_segments["segments"], 1):
            # Get start and end times in SRT format
            start_time = seconds_to_srt_timestamp(segment["start"])
            end_time = seconds_to_srt_timestamp(segment["end"])

            # Write SRT entry
            f.write(f"{i}\n{start_time} --> {end_time}\n{segment['text']}\n\n")

    print_info("SRT file generated", subtitles_output)
else:
    # Generate SRT content as a string
    srt_content = ""
    for i, segment in enumerate(aligned_segments["segments"], 1):
        # Get start and end times in SRT format
        start_time = seconds_to_srt_timestamp(segment["start"])
        end_time = seconds_to_srt_timestamp(segment["end"])

        # Append SRT entry to the string
        srt_content += f"{i}\n{start_time} --> {end_time}\n{segment['text']}\n\n"

    # Parse the SRT content directly
    subtitles = list(srt.parse(srt_content))

"""# Preprocessing - Text

## Text - Load SRT File
"""

if not experiment_mode:
    # Subtitles:
    with open(subtitles_output, "r", encoding="utf-8") as f:
        subtitles = list(srt.parse(f.read()))

"""## Text - Sentence Segmentation"""


def format_timedelta(timedelta_obj):
    """Formats a datetime.timedelta object into HH:MM:SS.mmm timestamp.

    Args:
        timedelta_obj: The datetime.timedelta object.

    Returns:
        A string representing the timestamp in HH:MM:SS.mmm format.
    """
    total_seconds = timedelta_obj.total_seconds()
    hours = int(total_seconds // 3600)  # Get hours
    minutes = int((total_seconds % 3600) // 60)  # Get minutes
    seconds = int(total_seconds % 60)  # Get seconds
    milliseconds = int((total_seconds % 1) * 1000)  # Get milliseconds

    return f"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}"


sentences = []
for i, segment in enumerate(subtitles):
    sentences.append({
        'base_idx': i,
        'start_time': format_timedelta(segment.start),
        'end_time': format_timedelta(segment.end),
        'sentence': segment.content
    })

df_sentences = pd.DataFrame(sentences,
                            columns=['base_idx', 'start_time', 'end_time',
                                     'sentence'])
sentences = df_sentences['sentence'].tolist()
notebook_mode_print(df_sentences)

"""## Text - Paragraph
combination of all subtitle parts.  

WhisperAI enhances transcription with basic punctuation.
"""

paragraph = reduce(lambda acc, seg: acc + seg.strip() + ' ', sentences, '')
parahraph_wordcount = count_words_without_punctuation(paragraph)

# Print the paragraph
notebook_mode_print(paragraph)
print(f"paragraph word count: {parahraph_wordcount}")
print_info("paragraph sample", paragraph)

"""## Text - Paragraph Summarized

##### Time Taken: ~1min
"""

print_info("Summarizing Paragraph")

paragraph_summarized = ""

if parahraph_wordcount > 0:
    # Model: Longformer Encoder-Decoder
    model_name = "allenai/led-base-16384"
    tokenizer = LEDTokenizer.from_pretrained(model_name)
    model = LEDForConditionalGeneration.from_pretrained(model_name)
    text = paragraph

    # Tokenization
    inputs = tokenizer(text, return_tensors="pt", max_length=16384,
                       truncation=True)

    # Calculate dynamic summary length
    summary_length_percentage = hyperparameters["auto_summary"][
        "summary_length_percentage"]
    min_summary_length = hyperparameters["auto_summary"]["min_summary_length"]
    max_summary_length = hyperparameters["auto_summary"]["max_summary_length"]

    input_length = len(inputs["input_ids"][0])
    summary_length = int(input_length * summary_length_percentage)
    summary_length = max(min_summary_length,
                         min(summary_length, max_summary_length))

    # Summary Generation
    summary_ids = model.generate(
        inputs["input_ids"],
        max_length=summary_length,
        min_length=min_summary_length,
        length_penalty=1.2,
        num_beams=4,
        early_stopping=True
    )

    paragraph_summarized = tokenizer.decode(summary_ids[0],
                                            skip_special_tokens=True)
    print_info("paragraph summarized", paragraph_summarized)

# Simple Metrics
print_info("Simple Metrics")

original_length = len(paragraph)
summary_length = len(paragraph_summarized)

print(f"original length: {original_length}")
print(f"summary length: {summary_length}")

if original_length > 0 and original_length > summary_length:
    summarization_ratio = (original_length - summary_length) / original_length
    print(f"Summarized/Original Length Ratio: {summarization_ratio:.2f}")
elif original_length == 0:
    print_info("WARN: 0 Words found in video")

"""# Text

## Metric 1: Sentence- Summarized Paragraph Relevancy (Cosine Similarity)

##### Time Taken: ~2min
"""

print_section("Metric 1: Sentence-Summarized Relevancy")

if parahraph_wordcount < hyperparameters["metric_1"]["min_words"]:
    skip_text_metrics = True
    print_info(f"Warning: Video contained only {parahraph_wordcount} words, \
  which did not meet minimum wordcount threshold to use this metric. Skipping.")

else:
    skip_text_metrics = False

if not skip_text_metrics:
    # Model Setup
    model_instructor_xl = SentenceTransformer(
        'hkunlp/instructor-xl',
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )

if not skip_text_metrics:
    instruction = "Represent the text for semantic similarity"

    # 3: Embedding (with Tokenization)
    paragraph_embedding = model_instructor_xl.encode(
        [(instruction, paragraph_summarized)],
        convert_to_tensor=True,
        max_length=4096,
        truncation=True
    )

    sentence_embeddings = model_instructor_xl.encode(
        [(instruction, s) for s in sentences],
        convert_to_tensor=True,
        max_length=4096,
        truncation=True
    )

"""Embedding Explanation  
The [CLS] (classification) token is often used in transformer models to represent the overall meaning or summary of the input sequence. By extracting its embedding, you're essentially obtaining a representation that captures the main point or essence of the paragraph.
"""

if not skip_text_metrics:
    # 4: Relevance scores
    relevance_scores = [
        torch.cosine_similarity(paragraph_embedding, sentence_embedding).item()
        for sentence_embedding in sentence_embeddings
    ]

    # Normalization: min-max normalization
    min_score = min(relevance_scores)
    max_score = max(relevance_scores)
    normalized_scores = [(score - min_score) / (max_score - min_score) for score
                         in relevance_scores]

    # round
    normalized_scores = [
        np.format_float_positional(score, precision=2, unique=False,
                                   fractional=False, trim='k') for score in
        normalized_scores]

# 5: Display Results
drop_if_exists(df_sentences, "metric_1_score")

if not skip_text_metrics:
    df_sentences.insert(0, "metric_1_score", normalized_scores)
else:
    df_sentences.insert(1, "metric_1_score", 0)

notebook_mode_print(df_sentences)

# Interactive Sheet for easy exporting
# from google.colab import sheets
# sheet = sheets.InteractiveSheet(df=df_sentences)

"""# Video

## Metric 2: Shot Detection

##### Time taken ~1min
"""

# 1: Detect Scenes

scene_list = detect(video_input,
                    ContentDetector(
                        threshold=hyperparameters["metric_2"]["threshold"],
                        min_scene_len=hyperparameters["metric_2"][
                            "min_scene_len"]
                    ))

notebook_mode_print(scene_list)

# 2: Segment Scoring
scene_segments = [(
                  end.get_seconds() - start.get_seconds(), start.get_timecode(),
                  end.get_timecode())
                  for start, end in scene_list]

# Convert to DataFrame
df_scenes = pd.DataFrame(scene_segments,
                         columns=["duration", "start_time", "end_time"])

total_duration = df_scenes["duration"].sum()

# Calculate score: row's duration / total duration - this is Normalization
df_scenes["normalized_score"] = df_scenes["duration"] / total_duration
df_scenes["normalized_score"] = df_scenes["normalized_score"].round(2)

# Min-Max scaling
df_scenes["score"] = \
    (df_scenes["normalized_score"] - df_scenes["normalized_score"].min()) / \
    (df_scenes["normalized_score"].max() - df_scenes["normalized_score"].min())
df_scenes["score"] = df_scenes["score"].round(2)

notebook_mode_print(df_scenes)


# Function to compute weighted scene score for each sentence
def compute_sentence_score(sentence_row):
    total_score = 0
    scene_number_start = -1
    scene_number_end = -1
    sentence_start = ts_to_s(sentence_row['start_time'])
    sentence_end = ts_to_s(sentence_row['end_time'])
    sentence_duration = sentence_end - sentence_start

    # find scene start and corresponding index
    for scene_idx, scene_row in df_scenes.iterrows():
        scene_score = scene_row['score']
        scene_start = ts_to_s(scene_row['start_time'])
        scene_end = ts_to_s(scene_row['end_time'])
        scene_duration = scene_row['duration']

        # case 1: Fully contained
        if sentence_start >= scene_start and sentence_end <= scene_end:
            scene_number_start = scene_idx
            scene_number_end = scene_idx

            total_score = scene_score
            break

        # case 2: start-contained, end extends
        elif sentence_start >= scene_start and sentence_start < scene_end and sentence_end >= scene_end:
            scene_number_start = scene_idx

            percentage = (scene_end - sentence_start) / sentence_duration
            total_score += scene_score * percentage

        # case 3: end-contained, start extends
        elif sentence_start <= scene_start and sentence_end <= scene_end and sentence_end > scene_start:
            scene_number_end = scene_idx

            percentage = (sentence_end - scene_start) / sentence_duration
            total_score += scene_score * percentage

        # case 4: mid part
        elif sentence_start <= scene_start and sentence_end >= scene_end:
            percentage = scene_duration / sentence_duration
            total_score += scene_score * percentage

    return total_score, scene_number_start, scene_number_end


# 3: Apply score to contained sentence and track scene_number

# Drop existing columns if needed
drop_if_exists(df_sentences, "metric_2_score")
drop_if_exists(df_sentences, "scene_number_start")
drop_if_exists(df_sentences, "scene_number_end")

# Insert columns for metric_2_score and scene_number
df_sentences.insert(1, "metric_2_score", 0)
df_sentences.insert(1, "scene_number_end", 0)
df_sentences.insert(1, "scene_number_start", 0)

if not skip_text_metrics:
    # Apply the function to get the score and scene_number
    df_sentences[['metric_2_score', 'scene_number_start',
                  'scene_number_end']] = df_sentences.apply(
        compute_sentence_score, axis=1, result_type='expand')

    df_sentences['scene_number_start'] = df_sentences[
        'scene_number_start'].astype(int)
    df_sentences['scene_number_end'] = df_sentences['scene_number_end'].astype(
        int)

# Replace sentences_df with scenes
else:
    print_info("Metric 1 Skipped Defaulting to Metric 2-only evaluation...")

    df_scenes_as_sentences = df_scenes.assign(
        base_idx=df_scenes.index,
        scene_number_start=df_scenes.index,
        scene_number_end=df_scenes.index,
        metric_1_score=0,
        metric_2_score=df_scenes.score,
        start_time=df_scenes.start_time,
        end_time=df_scenes.end_time,
        sentence=""
    )

    df_sentences = df_scenes_as_sentences

    # only select columns from df_sentences
    # df_scenes_as_sentences = df_scenes_as_sentences[df_sentences.columns]
    # df_sentences = pd.concat([df_sentences, df_scenes_as_sentences], ignore_index=True)

# Display the updated DataFrame
notebook_mode_print(df_sentences)

"""# Final Score - Metric Weighting"""

# Normalized Weigthed average
w1 = hyperparameters['metric_1']['weight'] if not skip_text_metrics else 0
w2 = hyperparameters['metric_2']['weight']
weight_sum = w1 + w2
w1_normalized = w1 / weight_sum
w2_normalized = w2 / weight_sum

# Metric 1 Apply
drop_if_exists(df_sentences, "metric_1_weighted")
df_sentences.insert(0, "metric_1_weighted", 1)
df_sentences['metric_1_score'] = df_sentences['metric_1_score'].astype(float)

df_sentences['metric_1_weighted'] = w1_normalized * df_sentences[
    'metric_1_score']

# Metric 2 Apply
drop_if_exists(df_sentences, "metric_2_weighted")
df_sentences.insert(0, "metric_2_weighted", 1)
df_sentences['metric_2_score'] = df_sentences['metric_2_score'].astype(float)

df_sentences['metric_2_weighted'] = w2_normalized * df_sentences[
    'metric_2_score']

# Metric Final Apply
drop_if_exists(df_sentences, "metric_final")
df_sentences.insert(0, "metric_final", 1)

df_sentences['metric_final'] = df_sentences['metric_1_weighted'] + df_sentences[
    'metric_2_weighted']
df_sentences["metric_final"] = df_sentences["metric_final"].round(2)

# Reorder
df_sentences = df_sentences[[
    'metric_final',
    'metric_1_weighted',
    'metric_1_score',
    'metric_2_weighted',
    'metric_2_score',
    'scene_number_start',
    'scene_number_end',
    'base_idx',
    'start_time',
    'end_time',
    'sentence'
]].copy()

notebook_mode_print(df_sentences)

"""### Deletion Metric"""

# Threshold
threshold = hyperparameters['deletion_metric']['threshold']

# Percentage
percentile = df_sentences['metric_final'].quantile(threshold)
filtered_df_to_keep = df_sentences[df_sentences['metric_final'] >= percentile]
filtered_df_to_delete = df_sentences[df_sentences['metric_final'] < percentile]

# Timestamps
# sample_timestamps = [('00:00:00.00','00:00:01.25'), ('00:00:08.766', '00:00:11.042')]
sentence_timestamps = list(
    zip(filtered_df_to_keep['start_time'], filtered_df_to_keep['end_time']))

notebook_mode_print(sentence_timestamps)

if not skip_text_metrics:
    threshold = hyperparameters['deletion_metric']['threshold']

    filtered_df = df_sentences.copy()

    # Count words of each sentence
    filtered_df['word_count'] = filtered_df['sentence'].apply(
        count_words_without_punctuation)

    # Target word count to keep (e.g., 80% of total words when threshold = 20%)
    total_word_count = filtered_df['word_count'].sum()
    target_word_count = round(total_word_count * (1 - threshold))

    # Sort the DataFrame by 'metric_final' (low to high score)
    filtered_df = filtered_df.sort_values(by='metric_final')

    # Initialize the variable for the current word count of remaining sentences
    current_word_count = total_word_count

    # Create a new column for keeping/deleting sentences
    filtered_df["threshold_keep"] = 1  # Default to keeping all sentences

    # Iteratively mark sentences for deletion until the target word count is reached
    for idx, row in filtered_df.iterrows():
        # if current_word_count <= target_word_count: # deleting 1 more
        if current_word_count - row[
            "word_count"] <= target_word_count:  # deleting 1 less
            break  # Stop once we've removed enough words

        # Update the current word count after marking the sentence
        current_word_count -= row["word_count"]

        # Mark sentence for deletion
        filtered_df.at[idx, "threshold_keep"] = 0  # 0 = delete, 1 = keep

    # Restore sort
    filtered_df = filtered_df.sort_values(by='base_idx')

    # Text to keep/delete
    filtered_df_to_delete = filtered_df[
        filtered_df['threshold_keep'] == 0].copy()
    filtered_df_to_keep = filtered_df[filtered_df['threshold_keep'] == 1].copy()

    # Timestamps
    sentence_timestamps = list(
        zip(filtered_df_to_keep['start_time'], filtered_df_to_keep['end_time']))

    notebook_mode_print(filtered_df)

"""### Text to Keep"""

notebook_mode_print(
    filtered_df_to_keep[['metric_final', 'start_time', 'end_time', 'sentence']])

text_to_keep = " ".join(filtered_df_to_keep['sentence'].tolist())
paragraph_trimmed = text_to_keep

notebook_mode_print(text_to_keep)

"""### Text to Delete"""

notebook_mode_print(filtered_df_to_delete[
                        ['metric_final', 'start_time', 'end_time', 'sentence']])

text_to_delete = " ".join(filtered_df_to_delete['sentence'].tolist())

notebook_mode_print(text_to_delete)

if not skip_text_metrics:
    words_kept = filtered_df_to_keep['word_count'].sum()
    words_delete = filtered_df_to_delete['word_count'].sum()

    notebook_mode_print(
        f"Text deletion: {round(words_delete / (words_kept + words_delete) * 100, 2)}%")
    notebook_mode_print(
        f"Sentence deletion: {round(len(filtered_df_to_delete) / (len(filtered_df_to_delete) + len(filtered_df_to_keep)) * 100, 2)}%")

"""# PostProcessing

#### Time Taken: ~1.5min
6min video: ~2min to process, ~30sec to download
"""

print_section("Postprocessing")


def skim_video(input_video, output_video, segments_to_retain):
    """
    Skims a video by keeping only the specified segments and removes others.

    Args:
        input_video (str): Path to the input video file.
        output_video (str): Path to the output video file.
        segments_to_retain (list of tuples): List of tuples where each tuple contains
                                             (start_time, end_time) in seconds to retain.
    """
    print_info("Processed video...")

    # Prepare the select filter for video (only select the specified ranges)
    video_select_filter = '+'.join([
        f"between(t,{start},{end})"
        for start, end in segments_to_retain
    ])

    # Prepare the select filter for audio (only select the specified ranges)
    audio_select_filter = '+'.join([
        f"between(t,{start},{end})"
        for start, end in segments_to_retain
    ])

    # Construct the ffmpeg command with the specified filters
    ffmpeg_command = [
        "ffmpeg",
        "-y",
        "-i", input_video,
        "-vf", f"select='{video_select_filter}',setpts=N/FRAME_RATE/TB",
        "-af", f"aselect='{audio_select_filter}',asetpts=N/SR/TB",
        "-threads", str(os.cpu_count()),
        "-preset", "ultrafast",
        output_video
    ]

    # Run the FFmpeg command and capture the output
    result = subprocess.run(ffmpeg_command, stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE)

    # Check if FFmpeg finished successfully or if there were errors
    if result.returncode != 0:
        print("FFmpeg Error:")
        print(result.stderr.decode())  # Print the error output
    else:
        print_info("Video processed successfully.")


def get_video_length(input_video):
    """Get the duration (length) of a video file using ffmpeg-python."""
    probe = ffmpeg.probe(input_video, v='error', select_streams='v:0',
                         show_entries='format=duration')
    return float(probe['format']['duration'])


def generate_keep_timestamps(timestamps_to_remove, video_length=None):
    """
    Given a list of timestamps to remove from a video, generates the list of timestamps to keep.

    Args:
        timestamps_to_remove (list of tuples): List of segments to remove (start_time, end_time) in seconds.
        video_length (float, optional): Total length of the video in seconds. If not provided, the last segment's end is used.

    Returns:
        list of tuples: Segments to keep.
    """
    # Sort the timestamps to remove by their start times (just in case they're out of order)
    timestamps_to_remove.sort()

    # Initialize the list of segments to keep
    timestamps = []

    # If the first removal starts after 0, keep from the start of the video to the first removal
    if timestamps_to_remove[0][0] > 0:
        timestamps.append((0.0, timestamps_to_remove[0][0]))

    # Now, for each consecutive pair of timestamps to remove, keep the time between them
    for i in range(len(timestamps_to_remove) - 1):
        end_of_previous_removal = timestamps_to_remove[i][1]
        start_of_next_removal = timestamps_to_remove[i + 1][0]

        # If there's a gap, keep that gap
        if end_of_previous_removal < start_of_next_removal:
            timestamps.append((end_of_previous_removal, start_of_next_removal))

    # If there is time left after the last removal, keep it
    if video_length is not None:
        last_end_time = timestamps_to_remove[-1][1]
        if last_end_time < video_length:
            timestamps.append((last_end_time, video_length))

    return timestamps


# Timestamp pre-processing
original_video_length = get_video_length(video_input)

# Dev mode: export shorter video
video_length = min(original_video_length,
                   video_export_max_length_seconds) if video_export_max_length_seconds > 0 else original_video_length

# Trim Method 1: Video with sentences to remove, removed
# timestamps_to_remove = list(map(lambda x: (ts_to_s(x[0]), ts_to_s(x[1])), sentence_timestamps))
# timestamps_to_keep = generate_keep_timestamps(timestamps_to_remove, video_length)

# Trim method 2: Video of only sentences to keep
timestamps_to_keep = list(
    map(lambda x: (ts_to_s(x[0]), ts_to_s(x[1])), sentence_timestamps))

# print(f"Timestamps to remove: {timestamps_to_remove}")
notebook_mode_print(f"Timestamps to keep: {timestamps_to_keep}")

# Skim Video
if not experiment_mode:
    skim_video(video_input, video_output_skimmed, timestamps_to_keep)

# Download
if not experiment_mode:
    if notebook_mode:
        print_info("Downloading video...")
        files.download(video_output_skimmed)

# Export original text
if export_original_text:
    paragraph_to_file(paragraph, filename_paragraph_original)

    if notebook_mode:
        files.download(filename_paragraph_original)

# Export trimmed text
if export_trimmed_text:
    paragraph_to_file(paragraph_trimmed, filename_paragraph_trimmed)

    if notebook_mode:
        files.download(filename_paragraph_trimmed)

# Export summarized text
if export_summarized_text:
    paragraph_to_file(paragraph_summarized, filename_paragraph_summarized)

    if notebook_mode:
        files.download(filename_paragraph_summarized)

# Simple Metrics
if not experiment_mode:
    print_section("Simple Metrics")

    original_video_length = get_video_length(video_input)
    print(f"Original Video Length: {original_video_length:.2f}s\n")

    skimmed_video_length = get_video_length(video_output_skimmed)
    print(f"Skimmed Video Length: {skimmed_video_length:.2f}s\n")

    summarization_ratio = (
                                      original_video_length - skimmed_video_length) / original_video_length
    print(f"Skimmed/Original Video Length Ratio: {summarization_ratio:.2f}")

"""### Experiment Export"""

if experiment_mode:
    df_sentences[['metric_final', 'start_time', 'end_time', 'base_idx',
                  'sentence']].to_csv(filename_experiment_sentences,
                                      index=False)

    with open(filename_experiment_hyperparameters, "w") as f:
        json.dump(hyperparameters, f, indent=2)

    print_info("Experiment files exported.")
